{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# installing important libraries\n",
        "!pip install pytesseract\n",
        "!pip install pdf2image\n",
        "!sudo apt install tesseract-ocr\n",
        "!apt-get install -y poppler-utils\n",
        "!pip install imbalanced-learn"
      ],
      "metadata": {
        "id": "r7RJCm2ZwNKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c87689c-f6b9-4095-9bf5-fc675cbd5c3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.16.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the library\n",
        "import tracemalloc\n",
        "import os\n",
        "import platform\n",
        "from tempfile import TemporaryDirectory\n",
        "from pathlib import Path\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pytesseract\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from pdf2image import convert_from_bytes\n",
        "import re,time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import spacy\n",
        "from sklearn import preprocessing\n",
        "import pickle\n",
        "#library that contains punctuation\n",
        "import string\n",
        "\n",
        "#loading the english language small model of spacy\n",
        "en = spacy.load('en_core_web_sm')\n",
        "sw_spacy = en.Defaults.stop_words\n"
      ],
      "metadata": {
        "id": "4UKifAInnfhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a2b73b-82e4-4da7-b570-831518dd6dbb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining function to use tracemalloc\n",
        "def tracing_start():\n",
        "    tracemalloc.stop()\n",
        "    print(\"nTracing Status : \", tracemalloc.is_tracing())\n",
        "    tracemalloc.start()\n",
        "    print(\"Tracing Status : \", tracemalloc.is_tracing())\n",
        "def tracing_mem():\n",
        "    first_size, first_peak = tracemalloc.get_traced_memory()\n",
        "    peak = first_peak/(1024*1024)\n",
        "    print(\"Peak Size in MB - \", peak)"
      ],
      "metadata": {
        "id": "TZB1HcGmVK6Q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We will convert the pdf data to a label dataframe with every 1st page with form type so that this problem will be easy to solve"
      ],
      "metadata": {
        "id": "MHOt7jNvMgjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create supervised data\n",
        "def supervised_df_creation():\n",
        "    # iterating over the folder to get the all the files\n",
        "    for file in os.listdir('/content/'):\n",
        "        # check if file ends with extension pdf then only continue with process\n",
        "        # this is a fail safe if program fails while processing the file because we can start again from last save point\n",
        "        if file.endswith('.pdf'):\n",
        "            # check if supervised data file exists\n",
        "            if os.path.exists('/content/file.csv'):\n",
        "                # read the file if file exist\n",
        "                df_data= pd.read_csv('file.csv')\n",
        "                # check if file name is the supervised file and if present then continue to next file\n",
        "                if file in list(df_data['file']):\n",
        "                    continue\n",
        "            print(file)\n",
        "\n",
        "            # return the pdf pages as image format so that we can convert it into text afterwards\n",
        "            #it Read in the PDF file at 500 DPI\n",
        "            pdf_pages = convert_from_path(file, 500)\n",
        "\n",
        "            # Iterate through all the pages stored above\n",
        "            for page_enumeration, page in enumerate(pdf_pages, start=1):\n",
        "\n",
        "                try:\n",
        "                    # intialize a list to get the image file\n",
        "                    image_file_list = []\n",
        "                    # create a dataframe for storing the type and first page as text\n",
        "                    pages_df = pd.DataFrame(columns=['file','text', 'type'])\n",
        "\n",
        "                    # we will only check first page because on 1st page only we will get the form type so no need of reading ,preprocessing all the pages of pdf\n",
        "                    if page_enumeration == 1:\n",
        "                        # save the pages so that we can refer it later\n",
        "                        with TemporaryDirectory() as tempdir:\n",
        "                            filename = f\"{tempdir}\\page_{page_enumeration:03}.jpg\"\n",
        "                            page.save(filename, \"JPEG\")\n",
        "                            image_file_list.append(filename)\n",
        "                        # open the text file\n",
        "                        # with open(text_file, \"a\") as output_file:\n",
        "                            # Open the file in append mode so that\n",
        "\n",
        "                        # Iterate from 1 to total number of pages\n",
        "                        for image_file in image_file_list:\n",
        "                            text = str(((pytesseract.image_to_string(Image.open(image_file)))))\n",
        "\n",
        "                        # we get  the form type from the data using regex matching\n",
        "                        # we do this to make the problem into supervised learning\n",
        "                        # if form is present we can extract the name righly else make it other\n",
        "                        if re.search('FORM\\s\\w+',text):\n",
        "                            form_type = re.search('FORM\\s\\w+',text)[0]\n",
        "                        elif re.search('Form\\s\\w+',text):\n",
        "                            form_type = re.search('Form\\s\\w+',text)[0]\n",
        "                        else:\n",
        "                            form_type = 'other'\n",
        "\n",
        "                        # after getting the form type we will update the dataframe\n",
        "                        pages_df = pages_df.append({'file': file,'text': text, 'type': form_type}, ignore_index=True)\n",
        "\n",
        "                        # if file exists then we will append the df else we will create new file\n",
        "                        # we do this becasue the data is large and it might crash the system so we can start again but it will start from last endpoint and not from the start\n",
        "                        if os.path.exists('/content/file.csv'):\n",
        "                            pages_df.to_csv('file.csv',mode='a',header=False)\n",
        "                        else:\n",
        "                            pages_df.to_csv('file.csv',mode='w')\n",
        "\n",
        "                        # del the variables\n",
        "                        del filename,page,image_file_list,pdf_pages,text\n",
        "                except Exception as err:\n",
        "                    print(err)\n",
        "\n",
        "\n",
        "# function to clean text removing new line character , extra spaces, remove punctuation and number\n",
        "def clean_txt(text):\n",
        "    text = re.sub(\"'\",\" \",text)\n",
        "    text = re.sub(\"(\\\\W)+\",\" \",text)\n",
        "    text = re.sub(\"\\n\",\" \",text)\n",
        "    text = re.sub(\"(\\\\d)+\",\" \",text)\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "\n",
        "    return punctuationfree\n",
        "\n",
        "\n",
        "# function to make word from character\\\n",
        "def join_text(text):\n",
        "    text = \" \".join([i for i in text])\n",
        "    return text"
      ],
      "metadata": {
        "id": "u9BwyS9tN4s-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling function to create supervised df and saving into file so that we can use it anytime we want to use and file size is only 220kb\n",
        "tracing_start()\n",
        "start = time.time()\n",
        "supervised_df_creation()"
      ],
      "metadata": {
        "id": "DTA-n1XlM-h6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82bc9b69-e534-4e44-8817-09a69c1f3a31"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nTracing Status :  False\n",
            "Tracing Status :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the file and resey the index\n",
        "df = pd.read_csv('/content/file.csv')\n",
        "df = df.reset_index(drop=True)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "pS7kIYlONXGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a03892-edd7-46d3-eae0-efa644c71d34"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0          file  \\\n",
            "0           0  01057890.pdf   \n",
            "1           0  01057240.pdf   \n",
            "2           0  01074669.pdf   \n",
            "3           0  00104263.pdf   \n",
            "4           0  00104564.pdf   \n",
            "\n",
            "                                                text      type  \n",
            "0  41/15/2001 09:51 FAX 2123065325 LISTING QUALIE...    Form 8  \n",
            "1  a 2QIF-1|$92-5 |\\n\\nFORM D UNITED STATES |\\n\\n...    FORM D  \n",
            "2   \\n\\nare, CONFIDENT IAL\\n\\n- UNITED STATES\\nSE...  Form 13F  \n",
            "3   \\n\\nras\\n\\nVow sryuyY\\n\\n    \\n \\n\\nOMB\\n\\nOM...  FORM 13F  \n",
            "4  ‘t3rbuN ny pL were:\\n\\n2114] /o1 FEB 5 2 be\\n\\...  Form 13F  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocessing the type column"
      ],
      "metadata": {
        "id": "wC53MLV1Pb89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing the type column\n",
        "\n",
        "# replacing wrong form type and making it correcct\n",
        "replace_values = {'Form ONITED': 'other', 'FORM LIMITED': 'other', 'FORM L3F': 'Form 13F', 'FORM TA': 'other','FORM X': 'FORM X-17','Form 1i3F': 'Form 13F', 'Form i3F':'Form 13F','Form UNITED': 'other','Form 1':'Form 13F','Form 6': 'other','FORM 6': 'other','FORM 8': 'other','Form 8': 'other','Form 11': 'Form 11-K'}\n",
        "df = df.replace({\"type\": replace_values})\n",
        "\n",
        "# making the type lower so that multiple Form 13F & FORM 13F can be same\n",
        "df['type'] = df['type'].str.lower()\n",
        "\n",
        "# encoding the type for better processing\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "df['type']= label_encoder.fit_transform(df['type'])\n"
      ],
      "metadata": {
        "id": "ImybXSHIO6je"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessig the dataframe\n",
        "# making all the text lower\n",
        "df['text'] = df['text'].str.lower()\n",
        "\n",
        "# remove all the unnecessary words\n",
        "df['text'] = df.text.apply(clean_txt)\n",
        "\n",
        "# we will stem the word\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# spliting text on blank space\n",
        "df['text'] = df['text'].str.split()\n",
        "\n",
        "# applying the stemmer on data\n",
        "df['text'] = df['text'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "# removing stop word\n",
        "for item in list(df['text']):\n",
        "    ietm_copy = item.copy()\n",
        "    for i in ietm_copy:\n",
        "        if i in sw_spacy:\n",
        "            item.remove(i)\n",
        "\n",
        "# join the data\n",
        "df['text'] = df.text.apply(join_text)"
      ],
      "metadata": {
        "id": "dx-lFmEVNwBG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#vectorize and train the model"
      ],
      "metadata": {
        "id": "8tTgr3NZQvMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the tfidf vectorizer so we can transform the data for machine learning\n",
        "# we use l2 regularization because it give more accuracy\n",
        "vectorizer = TfidfVectorizer(stop_words='english',norm='l2')\n",
        "tfidf = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# getting x and y for ML modelling\n",
        "X = pd.DataFrame(tfidf.toarray())\n",
        "y = df['type']\n",
        "\n",
        "# using the train test split function\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y , random_state=10,  test_size=0.25,  shuffle=True)"
      ],
      "metadata": {
        "id": "Myow9UrBQufB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42)\n",
        "# fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# saving the pickle file for future purpose\n",
        "with open('model_pkl', 'wb') as files:\n",
        "    pickle.dump(model, files)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print((metrics.accuracy_score(y_test, y_pred)*100))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1ZBFOB3RQMH",
        "outputId": "cab8f72f-52fe-45ce-cb42-26624bac5660"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92.3076923076923\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.97      0.97      0.97        40\n",
            "           3       1.00      1.00      1.00         1\n",
            "           4       0.82      0.90      0.86        10\n",
            "           5       1.00      1.00      1.00         5\n",
            "           6       0.86      0.67      0.75         9\n",
            "\n",
            "    accuracy                           0.92        65\n",
            "   macro avg       0.78      0.76      0.76        65\n",
            "weighted avg       0.94      0.92      0.93        65\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end = time.time()\n",
        "print(\"time elapsed {} milli seconds\".format((end-start)*1000))\n",
        "tracing_mem()"
      ],
      "metadata": {
        "id": "caJDf_8dTQNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7e2c08-9810-4f04-8e0c-faa307c94c39"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time elapsed 21303.054332733154 milli seconds\n",
            "Peak Size in MB -  21.30649185180664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lA1vgeQXWDYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}